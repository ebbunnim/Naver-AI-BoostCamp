{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day14_MHA.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPi3IRHqTXhPXZR7oPJQIMA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IiOGGqVrj6mB"},"source":["# Multi-Headed Attention"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7fGkdv8j6st","executionInfo":{"status":"ok","timestamp":1612435438818,"user_tz":-540,"elapsed":903,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"52302ddd-ad8a-4ce9-e564-5b200f84bf44"},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","import torch.nn.functional as F\r\n","%matplotlib inline\r\n","%config InlineBackend.figure_format='retina'\r\n","print(f'Pytorch version : {torch.__version__}')\r\n","device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n","print(f'device: {device}')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Pytorch version : 1.7.0+cu101\n","device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Aw4PCo21j6vh"},"source":["\r\n","# Scaled Dot-Product Attention (SDPA)\r\n","- Data $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the number data and $d$ is the data dimension (n-sequence length, d-임베딩된 벡터의 차원)\r\n","- Query and Key $Q, K \\in \\mathbb{R}^{n \\times d_K}$\r\n","- Value $V \\in \\mathbb{R}^{n \\times d_V} $\r\n","$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right)V \\in \\mathbb{R}^{n \\times d_V} $\r\n","\r\n","- 하나의 self attention\r\n","- 이거를 multi header attention으로 확장할 것\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlxMk333j6-F","executionInfo":{"status":"ok","timestamp":1612435439362,"user_tz":-540,"elapsed":1078,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"59d77c58-3d8d-4e70-8701-3df8eecd93d4"},"source":["class ScaledDotProductAttention(nn.Module):\r\n","    def forward(self,Q,K,V,mask=None):\r\n","        d_K = K.size()[-1] # key dimension\r\n","        scores = Q.matmul(K.transpose(-2,-1))/np.sqrt(d_K) # FILL IN HERE\r\n","        if mask is not None:\r\n","            scores = scores.masked_fill(mask==0, -1e9)\r\n","        attention = F.softmax(scores,dim=-1)\r\n","        out = attention.matmul(V)\r\n","        return out,attention\r\n","\r\n","# Demo run of scaled dot product attention \r\n","SPDA = ScaledDotProductAttention()\r\n","n_batch,d_K,d_V = 3,128,256 # d_K(=d_Q) does not necessarily be equal to d_V\r\n","n_Q,n_K,n_V = 30,50,50\r\n","Q = torch.rand(n_batch,n_Q,d_K)\r\n","K = torch.rand(n_batch,n_K,d_K)\r\n","V = torch.rand(n_batch,n_V,d_V)\r\n","out,attention = SPDA.forward(Q,K,V,mask=None)\r\n","def sh(x): return str(x.shape)[11:-1] \r\n","print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\r\n","       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\r\n","\r\n","# It supports 'multi-headed' attention\r\n","n_batch,n_head,d_K,d_V = 3,5,128,256\r\n","n_Q,n_K,n_V = 30,50,50 # n_K and n_V should be the same\r\n","Q = torch.rand(n_batch,n_head,n_Q,d_K)\r\n","K = torch.rand(n_batch,n_head,n_K,d_K)\r\n","V = torch.rand(n_batch,n_head,n_V,d_V)\r\n","out,attention = SPDA.forward(Q,K,V,mask=None)\r\n","# out: [n_batch x n_head x n_Q x d_V]\r\n","# attention: [n_batch x n_head x n_Q x n_K] \r\n","def sh(x): return str(x.shape)[11:-1] \r\n","print (\"(Multi-Headed) SDPA: Q%s K%s V%s => out%s attention%s\"%\r\n","       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["SDPA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n","(Multi-Headed) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 5, 30, 256] attention[3, 5, 30, 50]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NlVTB0DPj7Bg"},"source":["# Multi-Headed Attention (MHA)\r\n","\r\n","$\\text{head}_{\\color{red}i} = \\text{Attention}(Q {\\color{green}W}^Q_{\\color{red}i},K {\\color{green}W}^K_{\\color{red}i}, V {\\color{green}W}^V_{\\color{red}i}) $"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-wrN5Qzj7GT","executionInfo":{"status":"ok","timestamp":1612435439913,"user_tz":-540,"elapsed":1213,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"9a84adde-7b6d-4d47-a5ac-124054c33e58"},"source":["class MultiHeadedAttention(nn.Module):\r\n","    def __init__(self,d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=None):\r\n","        \"\"\"\r\n","        :param d_feat: feature dimension\r\n","        :param n_head: number of heads\r\n","        :param actv: activation after each linear layer\r\n","        :param USE_BIAS: whether to use bias\r\n","        :param dropout_p: dropout rate\r\n","        :device: which device to use (e.g., cuda:0)\r\n","        \"\"\"\r\n","        super(MultiHeadedAttention,self).__init__()\r\n","        if (d_feat%n_head) != 0:\r\n","            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\"%(d_feat,n_head)) \r\n","        self.d_feat = d_feat\r\n","        self.n_head = n_head\r\n","        self.d_head = self.d_feat // self.n_head\r\n","        self.actv = actv\r\n","        self.USE_BIAS = USE_BIAS\r\n","        self.dropout_p = dropout_p # prob. of zeroed\r\n","\r\n","        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\r\n","        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\r\n","        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\r\n","        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\r\n","\r\n","        self.dropout = nn.Dropout(p=self.dropout_p)\r\n","    \r\n","    def forward(self,Q,K,V,mask=None):\r\n","        \"\"\"\r\n","        :param Q: [n_batch, n_Q, d_feat]\r\n","        :param K: [n_batch, n_K, d_feat]\r\n","        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \r\n","        :param mask: \r\n","        \"\"\"\r\n","        n_batch = Q.shape[0]\r\n","        Q_feat = self.lin_Q(Q) \r\n","        K_feat = self.lin_K(K) \r\n","        V_feat = self.lin_V(V)\r\n","        # Q_feat: [n_batch, n_Q, d_feat]\r\n","        # K_feat: [n_batch, n_K, d_feat]\r\n","        # V_feat: [n_batch, n_V, d_feat]\r\n","\r\n","        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\r\n","        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\r\n","        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\r\n","        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\r\n","        # Q_split: [n_batch, n_head, n_Q, d_head]\r\n","        # K_split: [n_batch, n_head, n_K, d_head]\r\n","        # V_split: [n_batch, n_head, n_V, d_head]\r\n","\r\n","        # Multi-Headed Attention\r\n","        d_K = K.size()[-1] # key dimension\r\n","        scores = torch.matmul(Q_split,K_split.permute(0,1,3,2))/np.sqrt(d_K) # FILL IN HERE\r\n","        if mask is not None:\r\n","            scores = scores.masked_fill(mask==0,-1e9)\r\n","        attention = torch.softmax(scores,dim=-1)\r\n","        x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper\r\n","        # attention: [n_batch, n_head, n_Q, n_K]\r\n","        # x_raw: [n_batch, n_head, n_Q, d_head]\r\n","\r\n","        # Reshape x\r\n","        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\r\n","        # x_rsh1: [n_batch, n_Q, n_head, d_head]\r\n","        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\r\n","        # x_rsh2: [n_batch, n_Q, d_feat]\r\n","\r\n","        # Linear\r\n","        x = self.lin_O(x_rsh2)\r\n","        # x: [n_batch, n_Q, d_feat]\r\n","        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\r\n","               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\r\n","               'scores':scores,'attention':attention,\r\n","               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\r\n","        return out\r\n","\r\n","# Self-Attention Layer\r\n","n_batch = 128\r\n","n_src   = 32\r\n","d_feat  = 200\r\n","n_head  = 5\r\n","src = torch.rand(n_batch,n_src,d_feat)\r\n","self_attention = MultiHeadedAttention(\r\n","    d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device)\r\n","out = self_attention.forward(src,src,src,mask=None)\r\n","\r\n","Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\r\n","Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\r\n","scores,attention = out['scores'],out['attention']\r\n","x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\r\n","\r\n","# Print out shapes\r\n","def sh(_x): return str(_x.shape)[11:-1] \r\n","print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\r\n","print ()\r\n","print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\r\n","print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\r\n","print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\r\n","print ()\r\n","print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\r\n","print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\r\n","print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\r\n","print ()\r\n","print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\r\n","print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\r\n","print ()\r\n","print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\r\n","print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\r\n","print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\r\n","print ()\r\n","print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Input src:\t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Q_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","K_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","V_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Q_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","K_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","V_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","\n","scores:   \t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n","attention:\t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n","\n","x_raw:    \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","x_rsh1:   \t[128, 32, 5, 40]  \t= [n_batch, n_src, n_head, d_head]\n","x_rsh2:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Output x: \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZtCif4kcpDwR","executionInfo":{"status":"ok","timestamp":1612435440182,"user_tz":-540,"elapsed":1398,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}}},"source":[""],"execution_count":5,"outputs":[]}]}