{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2_word2vec.ipynb의 사본","provenance":[{"file_id":"1VTvW_xhCVskuAJokXST6RImd8CAZ7Pdr","timestamp":1613354098951}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"h3FAK0fz1kOr"},"source":["## **2. Word2Vec**\n","1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\n","2. CBOW, Skip-gram 모델을 각각 구현합니다.\n","3. 모델을 실제로 학습해보고 결과를 확인합니다."]},{"cell_type":"markdown","metadata":{"id":"u9FrxTPWIsct"},"source":["### **필요 패키지 import**"]},{"cell_type":"code","metadata":{"id":"QjroCdtwI9Rz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613354188248,"user_tz":-540,"elapsed":5932,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"f0e2d719-a62b-4b1d-b0d9-0d387766d71b"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Collecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n","Collecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n","\u001b[K     |████████████████████████████████| 460kB 41.6MB/s \n","\u001b[?25hCollecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 13.9MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: colorama, tweepy, JPype1, beautifulsoup4, konlpy\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nSP7aXfJIr3i"},"source":["from tqdm import tqdm\r\n","from konlpy.tag import Okt\r\n","from torch import nn\r\n","from torch.nn import functional as F\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from collections import defaultdict\r\n","\r\n","import torch\r\n","import copy\r\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qugro74yJASr"},"source":["### **데이터 전처리**"]},{"cell_type":"markdown","metadata":{"id":"Q36dfSRRJDtX"},"source":["데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  \r\n","학습 데이터는 1번 실습과 동일하고, 테스트를 위한 단어를 아래와 같이 가정해봅시다."]},{"cell_type":"code","metadata":{"id":"CLZ2f-lRJSus"},"source":["train_data = [\r\n","  \"정말 맛있습니다. 추천합니다.\",\r\n","  \"기대했던 것보단 별로였네요.\",\r\n","  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\r\n","  \"완전 최고입니다! 재방문 의사 있습니다.\",\r\n","  \"음식도 서비스도 다 만족스러웠습니다.\",\r\n","  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\r\n","  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\r\n","  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\r\n","  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\r\n","  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \r\n","]\r\n","\r\n","test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vReElaFSLBYL"},"source":["Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."]},{"cell_type":"code","metadata":{"id":"dTjlRzmWMDK_"},"source":["tokenizer = Okt()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DTUsX672icp"},"source":["def make_tokenized(data):\r\n","  tokenized = []\r\n","  for sent in tqdm(data):\r\n","    tokens = tokenizer.morphs(sent, stem=True)\r\n","    tokenized.append(tokens)\r\n","\r\n","  return tokenized"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-z0z6HD2rrX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613354203018,"user_tz":-540,"elapsed":7230,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"0aca09d2-4bc8-4f29-e91f-bac8ba744b79"},"source":["train_tokenized = make_tokenized(train_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:05<00:00,  1.85it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"51exEpI0Mc3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613354203020,"user_tz":-540,"elapsed":7088,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"9e8d571f-64e6-4069-8692-25f5fd2bceb1"},"source":["word_count = defaultdict(int)\r\n","\r\n","for tokens in tqdm(train_tokenized):\r\n","  for token in tokens:\r\n","    word_count[token] += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 53092.46it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gyvHAMAnMh1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613354203796,"user_tz":-540,"elapsed":773,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"3aad9008-2b0f-424d-bb63-e9177921c7d5"},"source":["word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\r\n","print(len(word_count))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["60\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DaK_i3zL2vO3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613354203797,"user_tz":-540,"elapsed":770,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"67f74103-5302-4862-c7ba-0d54972efda4"},"source":["w2i = {}\r\n","for pair in tqdm(word_count):\r\n","  if pair[0] not in w2i:\r\n","    w2i[pair[0]] = len(w2i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 60/60 [00:00<00:00, 78471.54it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LiGqiEGDL5B_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613354203797,"user_tz":-540,"elapsed":767,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"9a6a9c7c-1834-478c-a398-703cc1346480"},"source":["print(train_tokenized)\r\n","print(w2i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n","{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vXA5zaPPM3Wd"},"source":["실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."]},{"cell_type":"code","metadata":{"id":"s47ssyVt89t1"},"source":["class CBOWDataset(Dataset):\r\n","  def __init__(self, train_tokenized, window_size=2):\r\n","    self.x = []\r\n","    self.y = []\r\n","\r\n","    for tokens in tqdm(train_tokenized):\r\n","      token_ids = [w2i[token] for token in tokens]\r\n","      for i, id in enumerate(token_ids):\r\n","        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n","          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1]) # 주변 (윈도우)\r\n","          self.y.append(id) # 중심\r\n","\r\n","    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\r\n","    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n","\r\n","  def __len__(self):\r\n","    return self.x.shape[0]\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return self.x[idx], self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvInhQ33AMJv"},"source":["class SkipGramDataset(Dataset):\r\n","  def __init__(self, train_tokenized, window_size=2):\r\n","    self.x = []\r\n","    self.y = []\r\n","\r\n","    for tokens in tqdm(train_tokenized):\r\n","      token_ids = [w2i[token] for token in tokens]\r\n","      for i, id in enumerate(token_ids):\r\n","        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n","          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1]) # 중심\r\n","          self.x += [id] * 2 * window_size # 주변 \r\n","\r\n","    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\r\n","    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n","\r\n","  def __len__(self):\r\n","    return self.x.shape[0]\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return self.x[idx], self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyAGV5IUUba0"},"source":["각 모델에 맞는 `Dataset` 객체를 생성합니다."]},{"cell_type":"code","metadata":{"id":"5ep7Hm6oBWyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613355044294,"user_tz":-540,"elapsed":734,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"7912f953-63a1-41ba-cdf1-0c439e08930c"},"source":["cbow_set = CBOWDataset(train_tokenized)\r\n","skipgram_set = SkipGramDataset(train_tokenized)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 22894.67it/s]\n","100%|██████████| 10/10 [00:00<00:00, 13495.19it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1QSo73PoRyd9"},"source":["### **모델 Class 구현**"]},{"cell_type":"markdown","metadata":{"id":"jnnk44R6R28x"},"source":["차례대로 두 가지 Word2Vec 모델을 구현합니다.  \r\n","\r\n","\r\n","*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\r\n","*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\r\n"]},{"cell_type":"code","metadata":{"id":"b_HP1ISq5CWv"},"source":["class CBOW(nn.Module):\r\n","  def __init__(self, vocab_size, dim):\r\n","    super(CBOW, self).__init__()\r\n","    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n","    self.linear = nn.Linear(dim, vocab_size)\r\n","\r\n","  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n","  def forward(self, x):  # x: (B, 2W)\r\n","    embeddings = self.embedding(x)  # (B, 2W, d_w)\r\n","    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\r\n","    output = self.linear(embeddings)  # (B, V)\r\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQAUApww68MJ"},"source":["class SkipGram(nn.Module):\r\n","  def __init__(self, vocab_size, dim):\r\n","    super(SkipGram, self).__init__()\r\n","    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n","    self.linear = nn.Linear(dim, vocab_size)\r\n","\r\n","  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n","  def forward(self, x): # x: (B)\r\n","    embeddings = self.embedding(x)  # (B, d_w)\r\n","    output = self.linear(embeddings)  # (B, V)\r\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58cJalkDWYMT"},"source":["두 가지 모델을 생성합니다."]},{"cell_type":"code","metadata":{"id":"8vWUXEi8WeM-"},"source":["cbow = CBOW(vocab_size=len(w2i), dim=256)\r\n","skipgram = SkipGram(vocab_size=len(w2i), dim=256)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxP7qdtNWil1"},"source":["### **모델 학습**"]},{"cell_type":"markdown","metadata":{"id":"QVggZrQ4WpBS"},"source":["다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."]},{"cell_type":"code","metadata":{"id":"ygVdz5rSBeNu"},"source":["batch_size=4\r\n","learning_rate = 5e-4\r\n","num_epochs = 5\r\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n","\r\n","cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\r\n","skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekixqKB3X5C1"},"source":["첫번째로 CBOW 모델 학습입니다."]},{"cell_type":"code","metadata":{"id":"-d95qR7oC822","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613355280171,"user_tz":-540,"elapsed":11121,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"263156e5-1ef0-4167-9015-6d1d8ea9a6e6"},"source":["cbow.train()\r\n","cbow = cbow.to(device)\r\n","optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\r\n","loss_function = nn.CrossEntropyLoss()\r\n","\r\n","for e in range(1, num_epochs+1):\r\n","  print(\"#\" * 50)\r\n","  print(f\"Epoch: {e}\")\r\n","  for batch in tqdm(cbow_loader):\r\n","    x, y = batch\r\n","    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n","    output = cbow(x)  # (B, V)\r\n","\r\n","    optim.zero_grad()\r\n","    loss = loss_function(output, y)\r\n","    loss.backward()\r\n","    optim.step()\r\n","\r\n","    print(f\"Train loss: {loss.item()}\")\r\n","\r\n","print(\"Finished.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  6%|▋         | 1/16 [00:00<00:02,  5.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 1\n","Train loss: 4.76423454284668\n","Train loss: 3.877540111541748\n","Train loss: 4.697382926940918\n","Train loss: 4.866057872772217\n","Train loss: 4.158361911773682\n","Train loss: 4.261722564697266\n","Train loss: 4.393556118011475\n","Train loss: 4.680698871612549\n","Train loss: 4.943302154541016\n","Train loss: 4.737784385681152\n","Train loss: 4.804426670074463\n","Train loss: 3.6463236808776855\n","Train loss: 3.7916486263275146\n","Train loss: 4.352444648742676\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 78.24it/s]\n","100%|██████████| 16/16 [00:00<00:00, 705.83it/s]\n","100%|██████████| 16/16 [00:00<00:00, 694.64it/s]\n","100%|██████████| 16/16 [00:00<00:00, 665.89it/s]\n","100%|██████████| 16/16 [00:00<00:00, 686.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.224295616149902\n","Train loss: 4.728593349456787\n","##################################################\n","Epoch: 2\n","Train loss: 4.583186626434326\n","Train loss: 3.737147331237793\n","Train loss: 4.579419136047363\n","Train loss: 4.729487419128418\n","Train loss: 4.03863525390625\n","Train loss: 4.00961971282959\n","Train loss: 4.2638468742370605\n","Train loss: 4.5540900230407715\n","Train loss: 4.815417766571045\n","Train loss: 4.570962429046631\n","Train loss: 4.616819858551025\n","Train loss: 3.338893413543701\n","Train loss: 3.6600136756896973\n","Train loss: 4.2322998046875\n","Train loss: 4.074901103973389\n","Train loss: 4.562763214111328\n","##################################################\n","Epoch: 3\n","Train loss: 4.406665802001953\n","Train loss: 3.6002299785614014\n","Train loss: 4.463200569152832\n","Train loss: 4.595195293426514\n","Train loss: 3.920806884765625\n","Train loss: 3.769679546356201\n","Train loss: 4.136260986328125\n","Train loss: 4.430707931518555\n","Train loss: 4.692493438720703\n","Train loss: 4.410676956176758\n","Train loss: 4.438283920288086\n","Train loss: 3.054064989089966\n","Train loss: 3.5313496589660645\n","Train loss: 4.11549186706543\n","Train loss: 3.9286422729492188\n","Train loss: 4.401692867279053\n","##################################################\n","Epoch: 4\n","Train loss: 4.234747886657715\n","Train loss: 3.4667654037475586\n","Train loss: 4.348728179931641\n","Train loss: 4.4630937576293945\n","Train loss: 3.804865598678589\n","Train loss: 3.5423364639282227\n","Train loss: 4.010960102081299\n","Train loss: 4.310416221618652\n","Train loss: 4.574106216430664\n","Train loss: 4.256492614746094\n","Train loss: 4.269267559051514\n","Train loss: 2.7925186157226562\n","Train loss: 3.405691146850586\n","Train loss: 4.0019097328186035\n","Train loss: 3.7856645584106445\n","Train loss: 4.245324611663818\n","##################################################\n","Epoch: 5\n","Train loss: 4.067550182342529\n","Train loss: 3.3367490768432617\n","Train loss: 4.236001014709473\n","Train loss: 4.333115100860596\n","Train loss: 3.6908113956451416\n","Train loss: 3.327746868133545\n","Train loss: 3.8881020545959473\n","Train loss: 4.193097114562988\n","Train loss: 4.459633827209473\n","Train loss: 4.107891082763672\n","Train loss: 4.11003303527832\n","Train loss: 2.554281711578369\n","Train loss: 3.283066749572754\n","Train loss: 3.8914506435394287\n","Train loss: 3.6461310386657715\n","Train loss: 4.093616008758545\n","Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FDahBf6IX4py"},"source":["다음으로 Skip-gram 모델 학습입니다."]},{"cell_type":"code","metadata":{"id":"jJxGEusqFV5r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613355290866,"user_tz":-540,"elapsed":995,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"33a80943-9cc9-433e-c980-7e4c6b3c856a"},"source":["skipgram.train()\r\n","skipgram = skipgram.to(device)\r\n","optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\r\n","loss_function = nn.CrossEntropyLoss()\r\n","\r\n","for e in range(1, num_epochs+1):\r\n","  print(\"#\" * 50)\r\n","  print(f\"Epoch: {e}\")\r\n","  for batch in tqdm(skipgram_loader):\r\n","    x, y = batch\r\n","    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n","    output = skipgram(x)  # (B, V)\r\n","\r\n","    optim.zero_grad()\r\n","    loss = loss_function(output, y)\r\n","    loss.backward()\r\n","    optim.step()\r\n","\r\n","    print(f\"Train loss: {loss.item()}\")\r\n","\r\n","print(\"Finished.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 64/64 [00:00<00:00, 769.24it/s]\n","100%|██████████| 64/64 [00:00<00:00, 761.48it/s]\n","  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 1\n","Train loss: 3.935598850250244\n","Train loss: 4.18071174621582\n","Train loss: 4.385351181030273\n","Train loss: 3.981175184249878\n","Train loss: 4.382734298706055\n","Train loss: 4.338055610656738\n","Train loss: 4.1024909019470215\n","Train loss: 4.443660259246826\n","Train loss: 3.8431544303894043\n","Train loss: 4.591559410095215\n","Train loss: 4.209688186645508\n","Train loss: 3.74810791015625\n","Train loss: 4.243536949157715\n","Train loss: 4.557506561279297\n","Train loss: 4.170951843261719\n","Train loss: 4.279292583465576\n","Train loss: 3.961508274078369\n","Train loss: 4.395082473754883\n","Train loss: 4.10136604309082\n","Train loss: 3.8903419971466064\n","Train loss: 4.5862812995910645\n","Train loss: 4.630316734313965\n","Train loss: 3.9476840496063232\n","Train loss: 4.1631693840026855\n","Train loss: 4.583922863006592\n","Train loss: 4.150356292724609\n","Train loss: 3.9295263290405273\n","Train loss: 4.595941066741943\n","Train loss: 4.246572971343994\n","Train loss: 4.090461254119873\n","Train loss: 4.977676868438721\n","Train loss: 4.5876383781433105\n","Train loss: 3.8474578857421875\n","Train loss: 4.1296491622924805\n","Train loss: 3.881134033203125\n","Train loss: 4.083103656768799\n","Train loss: 3.6946849822998047\n","Train loss: 4.463474273681641\n","Train loss: 4.3602824211120605\n","Train loss: 4.330451488494873\n","Train loss: 4.115851402282715\n","Train loss: 4.511241436004639\n","Train loss: 4.258664131164551\n","Train loss: 4.254327774047852\n","Train loss: 4.500828742980957\n","Train loss: 4.2665486335754395\n","Train loss: 4.820995330810547\n","Train loss: 4.233554363250732\n","Train loss: 4.44267463684082\n","Train loss: 4.213138103485107\n","Train loss: 3.814838409423828\n","Train loss: 3.838461399078369\n","Train loss: 4.303941249847412\n","Train loss: 4.445390701293945\n","Train loss: 4.005686283111572\n","Train loss: 4.658095836639404\n","Train loss: 4.404080867767334\n","Train loss: 3.751634359359741\n","Train loss: 4.690746307373047\n","Train loss: 3.7535157203674316\n","Train loss: 4.762723922729492\n","Train loss: 4.186285495758057\n","Train loss: 4.299439430236816\n","Train loss: 4.1390910148620605\n","##################################################\n","Epoch: 2\n","Train loss: 3.9156148433685303\n","Train loss: 4.138116359710693\n","Train loss: 4.350408554077148\n","Train loss: 3.9218411445617676\n","Train loss: 4.349874019622803\n","Train loss: 4.298464775085449\n","Train loss: 4.06765079498291\n","Train loss: 4.416908264160156\n","Train loss: 3.81246280670166\n","Train loss: 4.555377006530762\n","Train loss: 4.181242942810059\n","Train loss: 3.7147316932678223\n","Train loss: 4.214365005493164\n","Train loss: 4.528331279754639\n","Train loss: 4.140052795410156\n","Train loss: 4.244451999664307\n","Train loss: 3.930056571960449\n","Train loss: 4.359982013702393\n","Train loss: 4.077083587646484\n","Train loss: 3.8632924556732178\n","Train loss: 4.481089115142822\n","Train loss: 4.538173675537109\n","Train loss: 3.8898329734802246\n","Train loss: 4.135576248168945\n","Train loss: 4.548580646514893\n","Train loss: 4.0912275314331055\n","Train loss: 3.8786628246307373\n","Train loss: 4.568323612213135\n","Train loss: 4.215238094329834\n","Train loss: 4.0637078285217285\n","Train loss: 4.94622802734375\n","Train loss: 4.564087867736816\n","Train loss: 3.8236513137817383\n","Train loss: 4.096892356872559\n","Train loss: 3.851790428161621\n","Train loss: 4.056717395782471\n","Train loss: 3.6394803524017334\n","Train loss: 4.4129228591918945\n","Train loss: 4.321042060852051\n","Train loss: 4.304689407348633\n","Train loss: 4.0855584144592285\n","Train loss: 4.491935729980469\n","Train loss: 4.194793224334717\n","Train loss: 4.218070983886719\n","Train loss: 4.380002021789551\n","Train loss: 4.16314697265625\n","Train loss: 4.728244304656982\n","Train loss: 4.175225257873535\n","Train loss: 4.413535118103027\n","Train loss: 4.18951940536499\n","Train loss: 3.782217264175415\n","Train loss: 3.7980244159698486\n","Train loss: 4.274670600891113\n","Train loss: 4.4154372215271\n","Train loss: 3.9716591835021973\n","Train loss: 4.622870445251465\n","Train loss: 4.341304302215576\n","Train loss: 3.7228598594665527\n","Train loss: 4.660345077514648\n","Train loss: 3.734744071960449\n","Train loss: 4.728928089141846\n","Train loss: 4.15791654586792\n","Train loss: 4.273283958435059\n","Train loss: 4.103432655334473\n","##################################################\n","Epoch: 3\n","Train loss: 3.896101236343384\n","Train loss: 4.095746040344238\n","Train loss: 4.315644264221191\n","Train loss: 3.8635549545288086\n","Train loss: 4.317172527313232\n","Train loss: 4.259551525115967\n","Train loss: 4.033445358276367\n","Train loss: 4.39029598236084\n","Train loss: 3.7820067405700684\n","Train loss: 4.519355773925781\n","Train loss: 4.152939796447754\n","Train loss: 3.681682825088501\n","Train loss: 4.1857123374938965\n","Train loss: 4.499284744262695\n","Train loss: 4.109329700469971\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:00<00:00, 723.11it/s]\n","100%|██████████| 64/64 [00:00<00:00, 765.91it/s]\n","  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.210082530975342\n","Train loss: 3.8988308906555176\n","Train loss: 4.325079917907715\n","Train loss: 4.053057670593262\n","Train loss: 3.8364081382751465\n","Train loss: 4.377152442932129\n","Train loss: 4.447363376617432\n","Train loss: 3.8326268196105957\n","Train loss: 4.108140468597412\n","Train loss: 4.513599395751953\n","Train loss: 4.033143043518066\n","Train loss: 3.8284125328063965\n","Train loss: 4.541130065917969\n","Train loss: 4.184264183044434\n","Train loss: 4.037203788757324\n","Train loss: 4.914881229400635\n","Train loss: 4.540597915649414\n","Train loss: 3.80001163482666\n","Train loss: 4.064507484436035\n","Train loss: 3.8226611614227295\n","Train loss: 4.030473709106445\n","Train loss: 3.5853731632232666\n","Train loss: 4.363476276397705\n","Train loss: 4.282433032989502\n","Train loss: 4.279162406921387\n","Train loss: 4.055452823638916\n","Train loss: 4.472754955291748\n","Train loss: 4.132181167602539\n","Train loss: 4.181849956512451\n","Train loss: 4.260842323303223\n","Train loss: 4.061119079589844\n","Train loss: 4.636878967285156\n","Train loss: 4.117508888244629\n","Train loss: 4.384546279907227\n","Train loss: 4.166077613830566\n","Train loss: 3.7502903938293457\n","Train loss: 3.758334159851074\n","Train loss: 4.2455902099609375\n","Train loss: 4.385972023010254\n","Train loss: 3.937875270843506\n","Train loss: 4.587830543518066\n","Train loss: 4.279576301574707\n","Train loss: 3.694512367248535\n","Train loss: 4.630151748657227\n","Train loss: 3.7160720825195312\n","Train loss: 4.695257186889648\n","Train loss: 4.129889011383057\n","Train loss: 4.247578144073486\n","Train loss: 4.0681257247924805\n","##################################################\n","Epoch: 4\n","Train loss: 3.8770499229431152\n","Train loss: 4.053609371185303\n","Train loss: 4.281060695648193\n","Train loss: 3.806361198425293\n","Train loss: 4.284631729125977\n","Train loss: 4.221321105957031\n","Train loss: 3.9998726844787598\n","Train loss: 4.363825798034668\n","Train loss: 3.7517895698547363\n","Train loss: 4.483498573303223\n","Train loss: 4.124782085418701\n","Train loss: 3.6489639282226562\n","Train loss: 4.1575775146484375\n","Train loss: 4.470364570617676\n","Train loss: 4.078784465789795\n","Train loss: 4.176191329956055\n","Train loss: 3.8678343296051025\n","Train loss: 4.290377616882324\n","Train loss: 4.0292863845825195\n","Train loss: 3.8096914291381836\n","Train loss: 4.27462100982666\n","Train loss: 4.357994079589844\n","Train loss: 3.776090621948242\n","Train loss: 4.080863952636719\n","Train loss: 4.478981018066406\n","Train loss: 3.9761486053466797\n","Train loss: 3.778794765472412\n","Train loss: 4.514357566833496\n","Train loss: 4.153651237487793\n","Train loss: 4.010951519012451\n","Train loss: 4.883636951446533\n","Train loss: 4.517167568206787\n","Train loss: 3.7765400409698486\n","Train loss: 4.032495975494385\n","Train loss: 3.7937498092651367\n","Train loss: 4.004369735717773\n","Train loss: 3.5324654579162598\n","Train loss: 4.315203666687012\n","Train loss: 4.24445915222168\n","Train loss: 4.253871917724609\n","Train loss: 4.025535583496094\n","Train loss: 4.453695774078369\n","Train loss: 4.0709123611450195\n","Train loss: 4.145652770996094\n","Train loss: 4.143501281738281\n","Train loss: 3.960625410079956\n","Train loss: 4.5470170974731445\n","Train loss: 4.06043004989624\n","Train loss: 4.355708122253418\n","Train loss: 4.142813682556152\n","Train loss: 3.7190608978271484\n","Train loss: 3.719393014907837\n","Train loss: 4.216700077056885\n","Train loss: 4.356993198394775\n","Train loss: 3.9043397903442383\n","Train loss: 4.552981376647949\n","Train loss: 4.2189459800720215\n","Train loss: 3.666592597961426\n","Train loss: 4.600167751312256\n","Train loss: 3.6974990367889404\n","Train loss: 4.66171407699585\n","Train loss: 4.102203369140625\n","Train loss: 4.222318649291992\n","Train loss: 4.033172130584717\n","##################################################\n","Epoch: 5\n","Train loss: 3.8584518432617188\n","Train loss: 4.011715412139893\n","Train loss: 4.2466630935668945\n","Train loss: 3.750308036804199\n","Train loss: 4.252254009246826\n","Train loss: 4.183781623840332\n","Train loss: 3.966930866241455\n","Train loss: 4.337498664855957\n","Train loss: 3.7218141555786133\n","Train loss: 4.447809219360352\n","Train loss: 4.096770763397217\n","Train loss: 3.6165771484375\n","Train loss: 4.129958629608154\n","Train loss: 4.441571235656738\n","Train loss: 4.04841947555542\n","Train loss: 4.14278507232666\n","Train loss: 3.837069034576416\n","Train loss: 4.2558794021606445\n","Train loss: 4.005768775939941\n","Train loss: 3.7831459045410156\n","Train loss: 4.17366361618042\n","Train loss: 4.27018404006958\n","Train loss: 3.720249652862549\n","Train loss: 4.053746700286865\n","Train loss: 4.444726467132568\n","Train loss: 3.920292377471924\n","Train loss: 3.7298269271850586\n","Train loss: 4.488000869750977\n","Train loss: 4.12339973449707\n","Train loss: 3.984951972961426\n","Train loss: 4.852495193481445\n","Train loss: 4.49379825592041\n","Train loss: 3.7532360553741455\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|██████████| 64/64 [00:00<00:00, 732.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.000858306884766\n","Train loss: 3.7650628089904785\n","Train loss: 3.9784064292907715\n","Train loss: 3.480867862701416\n","Train loss: 4.268176078796387\n","Train loss: 4.20712423324585\n","Train loss: 4.228814601898193\n","Train loss: 3.995807647705078\n","Train loss: 4.434756278991699\n","Train loss: 4.011079788208008\n","Train loss: 4.109465599060059\n","Train loss: 4.028143405914307\n","Train loss: 3.861844539642334\n","Train loss: 4.458781719207764\n","Train loss: 4.0040154457092285\n","Train loss: 4.327019691467285\n","Train loss: 4.1197285652160645\n","Train loss: 3.6885290145874023\n","Train loss: 3.6812026500701904\n","Train loss: 4.188002586364746\n","Train loss: 4.328497886657715\n","Train loss: 3.8710567951202393\n","Train loss: 4.5183281898498535\n","Train loss: 4.159461498260498\n","Train loss: 3.639101982116699\n","Train loss: 4.570396423339844\n","Train loss: 3.6790246963500977\n","Train loss: 4.628300189971924\n","Train loss: 4.074861526489258\n","Train loss: 4.197501182556152\n","Train loss: 3.9985713958740234\n","Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Pi0sbHV6dEOR"},"source":["### **테스트**"]},{"cell_type":"markdown","metadata":{"id":"WGarLWxXeJvz"},"source":["학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."]},{"cell_type":"code","metadata":{"id":"4A1wrl-L_RjF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613355297180,"user_tz":-540,"elapsed":1094,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"6c6c8c70-5e17-4efc-e1b9-36ff08957035"},"source":["for word in test_words:\r\n","  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n","  emb = cbow.embedding(input_id)\r\n","\r\n","  print(f\"Word: {word}\")\r\n","  print(emb.squeeze(0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Word: 음식\n","tensor([-1.0323e+00,  9.4741e-02, -1.6479e+00, -2.4390e+00, -9.3792e-01,\n","        -2.5977e-01,  1.4184e+00,  9.8140e-01, -8.7173e-01,  2.5267e-01,\n","        -3.0643e-01, -5.8586e-01,  1.2068e+00,  2.4510e-01,  2.9336e-01,\n","         9.8713e-01, -7.3095e-01,  3.2432e-01,  1.3086e-01, -1.4584e+00,\n","        -2.1182e+00,  6.5387e-01,  6.2431e-01,  7.4095e-01, -1.2376e+00,\n","         7.4692e-02, -1.4138e+00,  5.8394e-02,  3.9641e-01, -4.4608e-01,\n","        -1.7196e+00,  2.9443e-01,  9.0518e-02,  1.1679e+00,  1.3366e+00,\n","        -1.8578e+00, -4.2563e-01,  2.2517e-01, -6.2500e-02, -7.1083e-02,\n","        -1.3357e+00,  4.7396e-01,  6.2155e-01,  1.7937e+00,  4.0047e-02,\n","        -3.0487e-01, -2.6982e-01, -1.8118e+00, -1.5114e+00,  8.4694e-01,\n","         4.7915e-01, -7.9572e-01, -9.0195e-01,  7.8691e-01, -1.9914e+00,\n","         7.0706e-01,  2.3641e+00, -1.0699e-02, -8.3425e-01,  2.0945e+00,\n","         1.2797e+00,  8.9398e-01,  3.6953e-01,  4.1938e-01,  3.1600e-01,\n","        -1.4501e+00, -1.5972e+00, -1.2593e+00, -2.8200e+00,  3.8338e-01,\n","         1.1035e+00, -2.3028e-01,  4.9943e-01, -1.2543e+00,  1.5182e-01,\n","         1.1663e+00, -9.2854e-01,  1.6553e+00,  1.4955e+00,  2.3390e+00,\n","        -3.1494e-01,  7.4757e-01,  1.5058e-01,  1.4097e+00, -1.4582e+00,\n","        -1.9919e-01, -1.2490e+00,  1.3455e+00, -1.6879e+00,  1.8221e-01,\n","         6.6585e-01,  3.2816e-01,  2.1441e-01, -1.0962e+00,  3.1362e-01,\n","        -4.6199e-01,  1.3917e-01, -6.0740e-01, -1.7682e+00, -5.0896e-01,\n","         8.6127e-02, -2.4820e+00,  1.5645e+00,  1.2092e-01,  5.0020e-01,\n","         1.2146e-01, -1.1124e-02, -9.5202e-01, -1.6517e+00, -1.5857e+00,\n","        -4.3870e-01, -5.4036e-01, -1.7574e-01, -1.1313e+00,  7.6649e-01,\n","         3.0222e-01, -7.6753e-01,  6.7429e-01, -1.7029e+00, -1.8829e-01,\n","        -2.9702e-01,  5.5345e-01,  2.0198e+00,  8.7216e-02,  8.2834e-01,\n","         1.4457e+00,  1.4036e+00, -6.6902e-01,  2.1266e+00,  3.1930e-01,\n","         3.8564e-01, -7.4490e-01,  3.6516e-01,  7.3927e-01, -2.1062e-02,\n","        -4.0232e-01, -2.2068e-01,  1.4502e-01, -1.4471e+00, -9.0356e-01,\n","         8.7484e-01,  8.7300e-01, -4.8276e-01,  1.0253e-01, -9.1993e-02,\n","         8.3063e-03,  1.5174e-01,  1.4041e+00,  6.6271e-01, -1.5513e+00,\n","        -4.2557e-01,  1.0518e+00,  1.3791e+00,  1.1765e+00,  2.8155e-01,\n","        -9.2294e-01, -7.0721e-01, -4.5566e-01,  7.5952e-01, -7.7662e-01,\n","         2.2005e+00,  1.9688e+00,  3.8636e-04,  5.0698e-01, -1.2464e+00,\n","        -1.1271e+00,  1.1233e-01,  9.9113e-02, -8.6402e-01,  2.9841e-01,\n","        -3.0090e-01,  1.5497e+00,  1.3812e+00, -1.2346e+00,  8.1156e-01,\n","        -2.7671e-01, -5.4884e-01,  1.7908e-01,  4.2371e-01, -1.5739e+00,\n","        -3.9281e-01,  1.3943e+00, -1.8218e+00,  5.5672e-01,  1.7772e-01,\n","        -3.9058e-02, -5.0083e-01, -9.1274e-01, -2.0195e-01,  1.4960e+00,\n","         5.5765e-02,  1.6683e-01, -1.2351e+00,  2.4544e-03,  1.6633e+00,\n","        -8.6314e-01,  4.4322e-02, -4.8482e-01,  5.7132e-01,  2.4002e-01,\n","        -1.0260e+00,  2.9402e-01, -4.5899e-01,  1.4085e-01, -2.6122e-01,\n","        -7.8179e-01, -7.1180e-01,  6.1373e-01,  3.4620e-01,  1.5406e+00,\n","        -5.8962e-01, -1.7805e-01,  9.0894e-01,  1.7864e+00, -1.9489e-01,\n","         4.2275e-01,  2.3232e-01,  2.2819e+00,  7.4293e-01, -7.6190e-01,\n","         7.3651e-01,  1.0041e+00,  3.6662e-01, -1.7973e+00, -1.0307e-01,\n","        -1.2827e+00,  2.2253e-01,  1.0893e+00, -8.7685e-01,  2.9960e-01,\n","         3.5008e-01, -1.0741e+00,  1.4334e+00,  1.4334e+00,  9.5854e-03,\n","         2.6450e-01, -1.0182e+00,  1.0131e-01,  2.7299e-01,  2.0678e+00,\n","         4.5951e-01,  3.8842e-01, -1.2632e+00,  5.3441e-03,  7.8756e-01,\n","        -1.4723e+00,  3.0761e-01, -1.2288e-01, -2.9079e-01,  1.8705e+00,\n","        -2.0414e+00, -3.5296e-01,  2.1977e-01, -4.6344e-01, -2.1863e+00,\n","         1.4144e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 맛\n","tensor([-1.8955e+00, -6.7710e-01,  1.1625e+00,  1.6732e-01,  2.3060e+00,\n","        -3.8017e-01, -7.5975e-01, -3.7087e-01,  1.5120e+00, -1.3739e+00,\n","         4.9495e-01,  3.4697e-02, -3.9816e-01,  6.6688e-01,  2.4961e-01,\n","        -4.9469e-01,  4.1096e-01, -9.3983e-01,  9.9062e-01,  3.7020e-01,\n","         2.7900e-01,  1.9482e+00,  1.8901e+00, -4.4038e-01, -1.1785e+00,\n","         1.1788e+00, -7.4433e-01, -2.6840e-01, -6.5426e-01,  6.6743e-01,\n","        -3.8054e-01, -9.2417e-01, -5.2067e-01, -4.9955e-01,  1.1231e+00,\n","        -2.1181e-01, -5.7607e-01,  1.4876e+00, -2.2037e-01, -1.2914e-01,\n","         6.9128e-01,  5.1050e-02,  1.5125e-01,  5.6436e-01,  2.6939e+00,\n","        -1.1011e+00, -1.0746e+00,  1.7775e+00, -5.8234e-01,  7.3668e-01,\n","         1.1295e+00,  1.5261e+00, -3.9971e-01, -1.4668e-01, -6.7760e-01,\n","         3.3864e-01,  4.9646e-02, -1.9090e+00, -1.5502e+00, -2.9957e-01,\n","         1.6161e+00,  1.2122e-02, -4.7112e-01, -1.0905e+00, -9.3050e-01,\n","        -1.3282e-03,  3.4772e-02,  4.1230e-01,  1.0989e-01,  5.2143e-01,\n","        -4.0842e-03, -2.1777e-01, -7.4367e-01, -2.1894e-01,  1.6486e+00,\n","         9.4747e-01, -1.7035e-01, -1.8611e+00, -4.1079e-01,  4.7463e-01,\n","        -1.6972e+00,  1.7955e+00,  2.3947e-01,  1.9302e+00,  5.8377e-01,\n","        -1.5712e+00,  8.9954e-01,  5.3845e-01, -1.0896e+00,  9.3105e-01,\n","        -8.5184e-01, -4.7340e-01,  1.7570e-01, -3.6147e-01,  1.2251e+00,\n","        -6.6788e-01, -8.4720e-01, -1.0806e+00,  8.1420e-01,  5.1884e-01,\n","        -1.2367e+00, -2.6465e-01,  1.3567e+00, -7.1691e-01,  9.4198e-01,\n","         1.2333e-01,  4.0323e-02, -3.0664e-01,  1.5560e+00,  8.4215e-01,\n","        -1.7217e-01,  1.0678e+00, -8.0251e-01,  1.3233e+00,  1.9325e-01,\n","         2.1685e+00,  6.4194e-01, -5.5707e-01,  8.4899e-01, -7.4168e-01,\n","        -7.2264e-01,  3.4132e-02, -8.9193e-01, -1.9088e+00, -1.5195e-01,\n","        -3.1360e+00,  1.1758e+00,  1.1036e+00,  3.4633e-01, -1.4237e+00,\n","        -1.2783e+00,  1.2292e+00, -8.7375e-01,  9.5458e-01,  1.2596e+00,\n","        -7.1880e-01, -5.6244e-01,  6.3304e-01,  1.1386e-01, -2.9636e-02,\n","         1.1305e+00, -1.5824e+00, -6.9718e-01, -4.5807e-02, -7.0833e-01,\n","        -3.5872e-03,  2.1987e-02, -2.3911e+00,  1.4026e+00, -5.3311e-01,\n","        -8.3556e-01, -4.5561e-01,  9.4768e-01, -1.1595e+00, -1.4292e+00,\n","        -1.2600e+00, -1.9595e+00,  8.7060e-01, -2.1910e+00,  5.9930e-01,\n","         3.6308e-01, -8.8448e-01, -1.7450e+00,  6.5431e-01,  4.0367e-01,\n","        -9.9283e-01, -4.3310e-01, -1.5502e+00, -4.0696e-01, -2.7376e-01,\n","        -4.2611e-01, -7.5160e-01,  1.5068e+00,  3.0791e-01,  2.2060e+00,\n","        -1.9368e-01,  2.0692e+00, -2.4582e-01,  2.5474e-01,  4.9658e-01,\n","         5.4619e-01, -7.7924e-02,  1.8401e+00,  3.2624e-01, -2.9742e-01,\n","        -1.6971e-01,  3.2865e-02,  1.4669e+00,  5.7853e-01, -2.1033e-01,\n","         6.0955e-01,  8.0417e-02,  1.8194e+00,  1.1909e+00, -1.4990e+00,\n","         5.7737e-01,  8.5539e-01, -1.1991e+00,  2.8072e-01,  5.0519e-01,\n","         9.8065e-02,  4.7982e-01, -6.6452e-01,  1.6494e+00, -1.5100e+00,\n","        -2.0669e-01,  3.6153e-01, -3.3135e-03,  1.0227e+00, -5.4637e-03,\n","         5.5300e-01, -8.8149e-02, -1.1414e+00,  2.6490e+00,  1.5575e+00,\n","        -4.2279e-03, -5.1389e-01, -2.5259e-01,  1.1712e+00,  9.8908e-01,\n","         6.2863e-02, -1.8381e-01,  6.4441e-01, -9.5447e-01, -1.2345e+00,\n","         5.6076e-02,  1.1269e+00, -1.9041e+00,  1.0517e-01,  6.2465e-01,\n","        -1.2709e-01, -3.8828e+00, -7.6506e-01,  1.2594e-01,  1.2806e+00,\n","         1.2204e+00, -5.7316e-01, -1.6024e+00, -1.9474e-01, -4.4091e-01,\n","         6.9798e-01, -2.6898e-01, -2.6839e-02, -7.5806e-01, -9.5692e-01,\n","         1.1195e+00,  7.7439e-01,  1.2175e+00,  6.0735e-02,  2.1551e+00,\n","         9.7620e-01, -1.2839e+00,  1.3074e+00,  1.0065e+00,  1.5222e+00,\n","         1.6466e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 서비스\n","tensor([-1.3682e+00, -4.8749e-01,  1.0549e+00,  9.1773e-01,  1.8169e+00,\n","        -1.4421e-01,  1.5920e-01,  6.4869e-01, -2.2324e+00, -6.0846e-01,\n","         1.4832e+00,  2.6703e-01,  5.3752e-01, -9.2428e-01, -1.0656e+00,\n","        -1.8344e-01, -5.2031e-01, -1.9382e+00,  5.1945e-01, -4.9143e-03,\n","        -8.2782e-01, -1.4363e+00, -6.1057e-01,  5.5738e-01,  2.2412e-01,\n","         1.2785e-01, -1.1987e-01, -1.2065e-01, -4.5584e-01, -9.1700e-01,\n","        -1.2552e+00, -4.0659e-01, -5.8143e-03,  1.8011e-01,  9.7072e-01,\n","        -4.1357e-01, -1.0717e+00, -1.2482e+00,  2.1191e+00,  1.4475e-02,\n","         3.6489e-01,  1.7764e+00,  8.7371e-01, -2.5527e-01, -1.5087e+00,\n","         1.5497e+00,  2.9977e+00, -1.3286e-01, -4.1320e-01, -1.4635e+00,\n","        -1.7859e+00,  1.5462e+00, -7.9787e-01, -5.8959e-02, -1.2502e-01,\n","         5.7975e-01,  5.2191e-01,  2.8419e-01, -7.8643e-01,  5.5228e-01,\n","         6.6678e-01, -6.5040e-01, -6.1632e-01, -7.6790e-01, -1.9705e-01,\n","         6.9641e-01, -2.0486e-01, -9.6983e-01,  5.9238e-02, -1.1001e+00,\n","        -1.0696e+00,  1.3510e+00,  1.2668e+00, -1.3931e+00,  1.3138e+00,\n","        -2.0752e-01,  8.3215e-01, -7.5942e-02,  6.3030e-01,  7.5278e-01,\n","        -1.2460e+00,  2.0923e+00, -1.1823e+00, -7.5358e-02, -1.4987e+00,\n","         5.1248e-01, -4.0769e-01, -4.4044e-01,  2.1792e+00,  5.6101e-01,\n","         8.9619e-01,  1.1029e+00,  2.3463e-01,  1.2963e+00,  1.1301e+00,\n","         1.1327e+00,  9.8839e-01,  6.0178e-01, -1.6866e+00,  7.1847e-01,\n","        -2.0661e+00, -1.6748e-01,  1.3740e+00, -6.7627e-01, -4.6104e-01,\n","        -1.1943e-03, -3.7564e-01, -1.1305e+00,  2.8648e-01, -1.3623e+00,\n","         1.0055e+00,  7.9700e-01, -5.1721e-02,  1.3229e+00,  6.1728e-01,\n","        -8.0383e-01,  1.6734e+00, -1.2164e+00,  5.1463e-01, -6.8048e-01,\n","         1.8009e+00, -6.3194e-01, -2.3128e-01, -1.0010e-01,  1.3668e+00,\n","        -7.4278e-01,  2.5040e+00,  7.7418e-02,  2.4964e-01,  2.3801e+00,\n","        -1.9379e+00,  8.9413e-01,  8.9870e-01,  2.6865e+00,  4.9079e-02,\n","         6.0809e-01,  2.7328e-02, -1.4184e+00, -9.1600e-01,  4.0143e-01,\n","        -1.4546e+00,  7.6845e-01,  3.6076e-01, -2.2724e-01, -4.3762e-01,\n","        -1.1818e+00,  1.0796e-02,  4.7212e-01, -6.1097e-01,  1.4898e+00,\n","        -7.9208e-01,  1.1262e+00,  9.2801e-01,  1.4286e+00,  1.0718e+00,\n","         1.4141e+00,  2.4385e-01,  1.0474e+00, -1.4835e+00,  1.1592e+00,\n","        -1.1469e-01, -5.8992e-01,  2.6742e-01, -7.9035e-01, -4.6749e-01,\n","        -2.5683e-01,  9.7206e-01, -9.8977e-02,  1.0344e+00,  9.5726e-01,\n","         4.5465e-01,  2.2257e+00,  1.2616e+00,  1.2085e+00,  6.2233e-01,\n","         2.1866e-01,  1.1018e+00,  1.9514e+00,  1.2406e-01, -4.4490e-01,\n","        -8.6323e-01,  2.0331e+00, -1.6034e+00,  7.9667e-01,  6.0349e-01,\n","         2.3748e+00,  1.4500e+00,  6.8051e-02,  2.4060e-01,  3.9860e-01,\n","        -1.9853e-02,  1.5126e+00, -8.0206e-01,  4.4577e-01, -1.1445e+00,\n","        -4.9029e-01, -7.0788e-01, -4.0230e-01, -2.8701e-02,  1.8806e-01,\n","         1.3487e+00, -7.5905e-02, -1.6489e+00,  1.2634e-01, -3.0280e-01,\n","         3.6490e-02,  1.0448e+00, -8.5727e-01,  8.3454e-01,  1.5361e-01,\n","         9.1192e-02,  8.1815e-01,  2.1098e+00, -1.6342e+00, -1.6331e+00,\n","         1.3114e-02,  1.2418e+00, -1.2764e-01,  3.2787e-01,  9.7938e-01,\n","        -1.6155e+00, -2.3407e+00, -1.2033e-01, -6.9515e-01,  1.4834e+00,\n","         7.9552e-01, -7.6554e-02,  2.7828e-01, -1.4095e-01,  1.1238e+00,\n","        -5.4843e-01,  1.0125e+00, -2.3324e+00,  8.9084e-01,  4.8088e-01,\n","        -1.0361e+00, -1.3328e+00,  1.6498e-01,  9.7731e-01, -4.3440e-01,\n","         1.8867e-01,  3.3465e-01,  5.2314e-01, -1.6305e+00, -1.0736e+00,\n","        -1.0146e+00, -1.1437e+00, -9.1202e-01, -2.2866e-01,  5.3545e-02,\n","        -2.6900e-01, -1.5940e+00, -2.1143e+00,  9.2694e-01,  1.8050e+00,\n","        -1.0044e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 위생\n","tensor([ 1.5365,  1.1468, -0.1301, -0.2321, -0.0259, -0.6774,  1.8843, -0.3644,\n","         0.3027, -2.4719, -1.8096,  0.5234, -0.1324, -1.4281,  2.4081,  1.0613,\n","        -1.1263,  0.9454,  0.0564, -0.3915,  0.2717,  1.0112, -1.5181,  1.0522,\n","         0.9840, -0.5194, -1.3477,  1.1563,  1.8770,  0.9308, -0.1315, -0.0450,\n","         0.9432,  1.8189,  0.9900,  0.3163,  0.6064, -1.2374, -0.4782, -0.9331,\n","        -1.2107, -0.0629, -0.9788,  0.3098,  1.2836,  0.4532,  0.9861,  0.4080,\n","         0.1387,  1.0193, -2.4181, -0.3752, -0.2022, -0.0976,  1.9593, -0.3360,\n","        -1.5039, -1.0329,  0.8291, -1.4096,  1.0572,  0.3296,  1.7519, -0.6427,\n","        -0.9721,  0.8073, -1.6516, -1.2209,  0.2230, -0.0627,  0.6105,  2.1500,\n","        -0.9681, -2.5415,  0.1721, -0.0315,  0.8472, -0.7197, -0.2272,  0.0474,\n","        -0.3260, -1.4278,  0.7197,  0.0381, -1.0016,  0.8820, -1.1410, -1.3569,\n","         1.2491, -0.6013, -0.9336,  1.2180, -2.3419,  0.0085,  1.3097, -1.4085,\n","         2.3681, -0.9364, -1.4212, -1.4411, -1.4941, -2.0054, -1.0058, -1.1691,\n","        -0.3249, -1.5754,  0.4401,  0.4927, -0.0150,  1.1612, -1.8665, -1.6590,\n","         0.4257, -0.5996,  0.8126, -0.2284,  0.6788,  0.4802,  0.3999, -0.7688,\n","        -0.1334, -0.6720,  1.6253,  0.0674,  0.1309, -0.1425,  1.2066,  0.3756,\n","        -1.2913, -0.1843,  2.0157,  0.4359, -0.0764, -1.8767,  0.4311,  0.9011,\n","         0.5856,  0.3531, -0.7327,  1.0412,  1.0225,  0.5457, -0.2798, -0.2997,\n","        -1.1893, -2.2496,  0.6966,  0.7626,  1.5503,  0.2770, -0.1422,  0.3774,\n","        -0.1496,  1.2216, -0.1955,  0.2876,  1.0901,  0.7784,  0.5359,  0.2477,\n","         0.2863, -0.9028,  0.9715, -0.1258, -0.3225, -0.4314, -0.3323,  0.6945,\n","         1.4344, -2.1651, -0.5848,  1.8747, -0.7190, -1.1841, -1.0532,  0.1519,\n","        -0.5287,  1.3429, -1.5710,  1.8975, -0.4406,  0.9792, -0.9068, -0.9245,\n","        -0.1867,  0.4313, -0.5196, -1.7876,  2.5257,  0.2339, -0.0869,  1.3539,\n","         0.3820, -0.2518, -0.6431, -1.1874,  1.7688,  1.1633, -1.4274, -0.1568,\n","         0.1222,  0.3789,  1.4788, -0.2838,  0.3822, -0.4230, -0.5291,  1.7160,\n","         1.3792, -0.0848, -0.2107,  0.9607,  0.1692, -0.6714, -0.0848, -0.8598,\n","         0.0625,  1.3195, -1.0419, -1.7226, -1.1304, -0.8252,  0.4477, -1.4742,\n","         0.3499, -0.0225,  0.5534,  0.5786,  0.6692,  0.8544, -0.2813, -0.9719,\n","        -1.9901, -0.8105,  0.5485, -0.0569, -0.9762, -0.1212, -1.1244, -1.3623,\n","         0.6393, -1.0882,  0.7933,  0.4875,  0.0118,  1.0233, -0.5828,  2.1004,\n","        -0.0939, -1.7932,  1.5576,  1.8573, -1.3448,  0.6424, -0.3833, -0.1525],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 가격\n","tensor([ 1.2829e+00,  1.4799e+00,  3.5928e-01,  5.5936e-01, -4.3476e-01,\n","         1.1575e+00,  2.3456e-01,  4.7767e-01, -2.0042e-01, -6.6365e-01,\n","        -4.7208e-01,  6.8125e-01,  2.1466e-01,  1.0011e+00, -1.4635e+00,\n","        -2.0746e+00, -6.2329e-01,  8.1544e-01,  5.0900e-01, -1.7808e+00,\n","         1.1600e+00,  1.7229e-01,  4.9070e-02, -1.6104e+00, -5.8976e-01,\n","         1.4315e+00,  8.2047e-01, -1.3585e+00,  1.2189e-01, -1.2305e-03,\n","        -1.0791e+00,  3.8766e-01,  2.5056e+00,  3.0693e-01, -3.0942e-01,\n","        -1.0175e+00,  6.0891e-01, -6.9753e-01,  1.4517e+00,  1.1743e+00,\n","        -4.3624e-01, -1.8377e+00, -1.4368e+00,  1.8437e+00, -1.0485e+00,\n","        -1.1083e+00, -3.4498e-01,  1.1087e+00,  1.8822e-02,  6.9239e-02,\n","        -1.3905e+00,  1.8122e+00, -6.5168e-01, -1.8364e+00,  2.1939e-01,\n","        -1.8297e-01,  8.9083e-01, -1.0236e+00,  5.5439e-01,  2.5722e+00,\n","        -1.4183e-01, -1.6604e-01, -2.2198e+00,  1.7329e-01, -1.9803e+00,\n","        -6.7823e-02,  3.8933e-01,  6.8104e-01, -5.5128e-01, -1.0082e+00,\n","        -9.9463e-01, -2.7901e-01,  1.1530e+00, -4.5712e-01,  6.3262e-01,\n","         7.4346e-01, -4.7564e-01,  2.6585e-01, -3.8578e-01,  1.6185e+00,\n","         1.5087e-01,  1.4457e+00, -1.2930e+00, -1.4180e+00, -7.6853e-01,\n","        -7.1519e-01, -7.7648e-01, -2.0617e+00, -1.7011e+00, -1.4643e-01,\n","         5.7817e-01,  3.5447e-01, -3.2104e-01,  3.7369e-01, -4.0330e-01,\n","         6.3800e-01, -1.2369e+00, -2.0868e-01,  2.2858e+00, -4.1536e-01,\n","        -1.0344e+00,  6.8094e-01, -6.7528e-02,  6.3810e-01, -4.7798e-01,\n","        -9.8524e-01,  1.2237e+00,  2.8081e-02,  5.4243e-01,  1.6337e+00,\n","         1.0480e-01,  8.6238e-01, -9.4446e-01,  3.3934e-01, -1.2354e-01,\n","         2.0322e+00, -1.8758e-02,  1.6123e+00, -1.5335e+00,  1.5364e+00,\n","         4.2182e-01, -9.9657e-01,  7.0219e-01,  1.8550e-01,  6.1440e-01,\n","         2.8907e-01, -1.1733e+00,  1.5333e+00,  3.3068e-01,  2.2282e-01,\n","        -7.1954e-01, -8.6169e-02, -1.4750e+00,  8.9725e-01, -1.2010e+00,\n","        -8.2290e-01,  1.6584e+00, -1.7996e+00,  1.9179e+00,  2.7484e-01,\n","         7.5911e-01, -5.2266e-01, -7.1455e-01,  1.3193e+00,  2.7067e-01,\n","        -6.4772e-01, -1.7752e-01,  8.7082e-02,  1.8402e+00, -4.4359e-02,\n","         7.5938e-01, -5.3463e-02,  6.8486e-01, -1.8965e-01,  1.2237e+00,\n","        -1.3431e+00, -6.0184e-01, -7.9071e-01, -4.7382e-01,  1.3557e+00,\n","         2.0433e+00, -1.1186e+00,  6.4187e-02, -2.2869e-01, -8.6715e-01,\n","         6.3413e-01,  2.5087e-01, -1.9038e+00,  1.0223e+00, -1.3879e+00,\n","         1.2261e+00,  2.0173e+00, -4.0689e-02,  3.7109e-01,  1.9366e-01,\n","        -1.5155e+00,  2.4682e-01,  7.4934e-01, -5.4497e-01, -1.1428e-01,\n","         1.2645e+00, -8.3623e-01,  1.6661e-01, -9.8553e-01, -2.4424e-01,\n","        -7.2600e-01,  9.4248e-01,  7.1267e-01,  4.1447e-01,  6.1532e-01,\n","         8.6244e-01, -3.0268e-01,  1.1503e+00, -7.0105e-01, -4.9427e-01,\n","        -3.0782e-01,  1.4451e+00, -1.1142e+00, -8.7716e-01, -1.3894e+00,\n","        -6.6886e-01,  3.9641e-01, -6.0268e-02, -3.2551e-02, -1.0313e+00,\n","         1.7092e+00,  1.1550e+00,  7.5271e-01,  7.8186e-01,  6.2233e-01,\n","        -1.3430e+00, -1.3383e+00, -1.2940e+00, -9.7932e-01,  8.9770e-01,\n","        -2.2118e-01, -9.9059e-01, -2.8532e-01,  4.9957e-01,  5.1402e-01,\n","        -1.2083e+00,  5.0876e-01,  1.5799e+00, -3.9555e-01,  9.1280e-01,\n","         1.9285e+00, -2.5723e+00, -2.3825e+00, -6.4540e-01,  1.0722e+00,\n","        -4.3214e-01, -6.2831e-01, -8.9080e-01,  4.3948e-01,  3.0515e-01,\n","        -1.1411e+00,  2.8267e-01,  2.4704e-01,  1.0295e+00, -4.8906e-01,\n","        -1.0419e+00,  3.1514e-01,  6.0946e-01, -1.6279e-01,  8.9328e-01,\n","        -1.7984e+00, -1.1507e+00,  1.2191e+00, -9.9942e-02,  2.0839e-01,\n","        -1.3008e-01, -1.2920e-01, -4.5298e-01,  3.1182e-01,  2.4774e-01,\n","        -4.1485e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_l5cPRZZe-R4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613355297181,"user_tz":-540,"elapsed":920,"user":{"displayName":"JiYoung Shin","photoUrl":"","userId":"11190703619360472168"}},"outputId":"245bc5f7-f983-4c29-9e53-99149e783779"},"source":["for word in test_words:\r\n","  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n","  emb = skipgram.embedding(input_id)\r\n","\r\n","  print(f\"Word: {word}\")\r\n","  print(emb.squeeze(0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Word: 음식\n","tensor([-9.1191e-01,  6.6738e-01, -1.0075e+00,  5.3786e-01,  1.7338e+00,\n","        -1.1773e+00, -1.1951e+00,  4.0240e-01, -1.4058e+00, -7.2768e-01,\n","        -5.0139e-01, -1.6413e-01,  7.8513e-01,  1.2303e+00, -1.1413e+00,\n","         1.1255e-01,  1.4635e+00,  1.3380e+00, -1.2341e+00, -3.0908e-01,\n","        -1.4730e-01,  1.5394e+00, -1.8612e+00, -5.8801e-01, -2.3625e-01,\n","         8.6905e-01, -1.0273e+00,  2.5554e-02,  4.1589e-01, -1.3586e-01,\n","         1.6397e+00, -2.1654e+00, -1.0361e+00,  1.1095e+00,  9.5384e-01,\n","        -2.8819e-01,  2.1022e+00, -1.7125e+00, -1.3131e+00, -1.1008e-03,\n","        -7.7169e-01,  1.1071e-01, -3.7384e-01,  9.1547e-01,  2.5213e-03,\n","        -5.9584e-01,  1.0911e-01,  3.5029e-02, -3.0037e-01,  1.1482e+00,\n","         6.0252e-01, -4.7165e-01, -3.1365e-01, -2.5076e-01,  2.2484e-01,\n","         7.8851e-03, -6.0370e-01,  1.8710e-01,  1.2705e-01, -1.1709e+00,\n","        -3.8019e-01, -1.0206e-01, -2.7376e-01, -1.0836e+00,  5.7283e-01,\n","        -1.5809e+00, -1.6338e-01, -6.5261e-01,  1.2105e-01, -1.9408e-01,\n","        -2.0220e+00, -5.0804e-01, -3.7682e-01,  5.5905e-01, -3.3615e-01,\n","         3.6550e-01, -5.7861e-01,  1.6187e+00, -1.3008e+00,  9.7135e-01,\n","        -4.9093e-01, -3.0722e-01,  1.4750e-01,  1.1531e+00,  1.0381e+00,\n","        -3.5621e-01, -3.5835e-01,  9.5292e-01,  7.5929e-01, -3.9190e-01,\n","        -9.1924e-02, -8.2536e-01, -3.1484e-01,  1.2189e+00, -3.6784e-01,\n","         5.9391e-01,  1.0054e+00,  1.0794e+00,  2.0648e-01,  8.4895e-02,\n","         3.5052e-01, -5.5984e-01,  8.8716e-01, -1.3345e+00,  6.0631e-01,\n","         1.0033e+00,  6.2258e-01, -1.1848e+00,  5.4501e-01,  1.7611e+00,\n","         4.1484e-01, -7.5207e-02, -1.7255e-01, -8.3663e-01,  3.3249e-01,\n","         8.1883e-01,  1.0394e+00, -4.7455e-01, -2.0541e-01,  6.4742e-01,\n","         8.0910e-01, -4.8102e-01, -3.2042e-01,  2.3449e-01, -1.7054e-01,\n","        -5.4210e-01, -7.0964e-01,  1.9468e-03, -8.1483e-01, -4.4601e-01,\n","        -1.9961e+00, -4.5087e-01,  3.3801e-01, -7.1147e-01, -2.2378e-01,\n","         2.7682e-01,  2.1981e-01,  1.6429e+00, -2.6469e-01,  1.2707e+00,\n","        -8.4102e-01,  8.9344e-01,  3.6598e-01, -1.1151e+00,  5.7471e-01,\n","         4.1538e-01, -4.4257e-01, -1.0476e+00, -1.9344e-01,  9.5831e-01,\n","         1.2789e+00,  1.5284e+00,  1.8127e+00,  3.7655e-01,  1.2215e+00,\n","         1.2570e+00,  8.0261e-01, -1.0222e-01, -5.2204e-01, -2.4178e-01,\n","         3.6730e-01, -9.4652e-01, -2.7201e-01,  5.9249e-01, -9.6497e-01,\n","         4.4377e-01, -1.0882e+00, -2.4588e-01,  1.2134e+00, -1.3164e+00,\n","         1.9192e+00,  6.3714e-01, -7.8487e-01,  1.0728e+00,  8.1418e-02,\n","         1.2179e+00, -8.0122e-01,  5.5657e-01,  8.7575e-01,  7.4732e-02,\n","        -2.6791e-01,  1.0675e-01, -3.1835e-01,  3.5201e-01,  3.2687e-01,\n","        -7.6901e-01, -4.1976e-01, -3.4625e-01, -4.8957e-01,  4.6555e-01,\n","         8.6183e-02,  6.8404e-01,  9.7496e-01, -1.2092e+00,  4.9197e-01,\n","         1.7399e+00, -2.2583e+00, -8.4518e-01,  1.0106e+00, -8.9275e-01,\n","        -1.3046e-01,  1.1725e+00,  1.2534e+00,  9.9178e-01,  4.6947e-01,\n","         1.5240e+00, -1.0140e+00,  2.7397e-01, -5.0699e-01,  2.3913e-01,\n","         5.0592e-03, -3.4923e-01,  1.7609e+00,  9.9653e-01, -2.2679e+00,\n","        -8.3995e-01, -1.5041e+00,  1.1102e+00,  8.5221e-01,  5.1197e-02,\n","        -1.2979e+00,  4.9784e-01, -3.9415e-01,  4.6995e-01,  1.2495e+00,\n","         9.2693e-01, -2.7058e-01, -7.5725e-02, -1.4741e+00,  7.5836e-01,\n","        -2.3253e-01, -1.6052e+00, -1.3816e+00, -3.2472e-02, -1.2892e+00,\n","         5.0961e-01,  1.1495e+00,  1.7508e+00,  2.0807e-01,  1.6364e+00,\n","         3.2359e-02,  1.3044e-01, -4.2781e-01, -2.2398e-01, -2.6043e-01,\n","         5.2848e-01,  7.1012e-01,  1.9683e+00,  3.5287e-01,  8.7656e-01,\n","         1.6952e+00,  8.7037e-01, -2.3188e-01,  1.3802e+00,  4.1550e-01,\n","        -1.4893e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 맛\n","tensor([ 0.7948, -0.3452,  1.7260,  0.2980, -0.1982, -1.2081,  0.6543, -1.1733,\n","        -1.0518,  0.8625, -0.3981, -1.1367,  1.3639, -0.2495, -0.3185,  1.6130,\n","        -0.5351,  0.8382, -0.4118,  1.3372, -0.6812,  0.2292, -0.5180, -0.9583,\n","        -1.6221, -0.1596,  1.1979, -1.5720,  1.2099,  2.0313,  1.7638,  0.5248,\n","         1.0195, -0.6980,  1.0014, -0.3805, -0.0114, -0.6287, -0.4357, -1.0776,\n","         1.6971,  1.2768,  0.5536,  0.9843, -0.4400, -0.4091,  1.5170,  0.9999,\n","        -2.0433, -0.9720,  0.6282,  2.0025,  1.2209,  0.1651,  1.8292,  1.0416,\n","         0.9713, -0.9974, -0.2000, -0.8816, -1.2194, -0.4602,  0.1618,  1.2969,\n","         0.5149,  0.8580, -0.5823, -0.9342, -0.6390, -0.3077,  2.0687, -2.1551,\n","         0.8046, -1.2873,  0.2474, -0.5572,  1.3090, -0.3777, -1.8853, -2.1627,\n","        -0.8690,  0.3158,  0.0371, -0.5400, -0.7800,  0.4749, -0.4066, -0.8085,\n","         2.5386,  0.8215,  0.5386,  0.1290,  1.1542, -0.7458, -0.3232,  0.0099,\n","        -0.9847,  0.2705, -1.2733,  0.7040, -2.6605, -1.7479,  1.2788,  1.0129,\n","         0.3051,  0.1585,  0.7767,  0.3836, -0.4616, -1.4280, -1.1567, -0.2126,\n","         0.0171,  1.0555, -1.3039, -0.1955, -0.2679, -0.1953, -1.1311,  0.7775,\n","        -0.2604,  0.1842,  0.8972,  0.4623,  0.6859,  1.0378, -0.7833, -0.0410,\n","         0.2734, -2.5045,  0.0160, -0.0930, -0.2398,  0.5906,  0.1327, -0.1477,\n","        -0.4517,  0.2378,  1.7844, -0.3645,  2.5670, -0.8255,  0.4824,  1.6926,\n","        -1.8532,  1.0322, -0.2062, -0.0327, -1.0970, -1.0952, -0.6303,  0.4117,\n","         1.8616, -0.9501, -0.5638, -2.6084, -0.1561, -0.1651, -1.0090, -1.1571,\n","        -0.6150, -1.0480, -1.7888,  0.3093, -1.0109, -0.1427,  0.9169,  0.9787,\n","         0.7890, -1.6912,  0.1851, -0.4834, -0.0781,  0.2711, -0.4797,  0.6852,\n","         0.7269, -0.5743, -0.5786, -2.0280,  1.4301, -0.0187, -3.5689,  1.4990,\n","         1.1227, -0.0991,  0.5255, -0.5310,  0.4650, -1.1156,  1.2029,  0.4784,\n","        -0.5872, -0.0749,  0.6652,  0.0825, -0.8924, -0.3076,  0.5878,  0.0576,\n","        -1.3360, -0.8551, -2.1697,  0.9099, -1.5134, -1.2181,  0.5510, -1.2230,\n","         1.4584,  0.2927, -1.2931,  0.3788,  0.5304, -0.5383,  1.6565,  0.9643,\n","        -0.3805, -0.5300, -1.4820,  0.8089, -0.9107,  2.1123, -0.7202,  1.5228,\n","        -0.5196,  1.1646,  0.1923,  0.6576,  0.1830,  0.7047,  0.3057, -1.1174,\n","         0.4235, -1.0479, -1.9354,  1.3579,  1.4228,  0.2085,  2.2782,  0.6781,\n","        -0.1862, -0.7052, -1.3235,  2.0314, -0.2896, -0.0539,  0.7618,  0.4302,\n","        -0.6980, -1.0676,  0.1963,  0.1616, -1.1426,  0.0432,  0.1041, -2.0124],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 서비스\n","tensor([-3.6972e-01, -1.4863e+00,  3.7342e-01,  5.7809e-01,  2.2428e-01,\n","         1.4373e-01, -8.7880e-01,  2.0530e+00, -7.0429e-02,  3.9168e-01,\n","         1.2797e+00, -3.0915e-01, -1.5610e+00, -2.2207e-01,  1.3690e+00,\n","         7.0107e-01,  1.2899e+00,  3.5350e+00, -2.6870e-01,  1.2992e+00,\n","         1.4470e+00,  3.7878e-01,  1.8916e+00, -9.7301e-01,  2.0374e+00,\n","        -6.5197e-01,  2.0140e+00, -4.7455e-01,  4.8257e-01,  1.0297e+00,\n","        -1.6222e+00,  5.5203e-01, -2.3974e-01, -8.4690e-01,  1.6664e+00,\n","         3.8600e-01, -5.2093e-01,  1.7652e+00, -9.8433e-01, -1.1510e+00,\n","         5.6492e-01, -1.5251e+00, -9.5127e-01,  1.2686e+00, -2.8717e-01,\n","         1.0616e+00, -7.8353e-01, -1.3543e+00, -1.7692e+00,  1.7989e+00,\n","         3.5157e-01,  8.0047e-01, -9.1938e-02, -2.0259e-01, -2.5408e-01,\n","         3.0289e+00,  1.7174e+00,  1.4357e-01,  4.4846e-01,  7.5882e-01,\n","        -7.2252e-01, -1.3506e+00,  5.0596e-01,  2.5109e-01, -2.2962e+00,\n","        -4.1483e-01,  2.0877e+00, -1.3903e-01,  3.1373e-01, -1.0460e+00,\n","         1.6410e+00, -7.6897e-01,  1.2450e+00,  2.6845e-01,  1.6482e+00,\n","         1.2187e+00,  9.5768e-01,  2.1336e-01, -4.1881e-01,  1.9519e-01,\n","        -4.7288e-01,  3.9587e-01, -6.2846e-01, -1.8706e+00, -2.2563e-01,\n","         1.8414e-01, -1.1333e+00,  1.1674e+00,  1.6243e+00,  4.0399e-02,\n","        -1.5030e+00, -4.1228e-01, -7.6676e-02,  8.4779e-01,  1.2373e+00,\n","         7.7699e-01, -8.9195e-02, -8.6832e-01, -1.1499e+00, -1.0437e+00,\n","        -3.8307e-01,  2.1000e+00, -2.2964e-01, -1.5660e+00,  3.3038e-01,\n","         1.2704e+00,  1.3316e+00,  8.3333e-01,  9.2649e-01,  9.8030e-01,\n","        -1.4066e+00, -1.2755e-01, -2.8204e-01, -8.3377e-02, -3.2903e-01,\n","        -6.1507e-01,  1.0628e+00,  3.0628e-01, -4.9203e-01,  9.4905e-01,\n","         1.6553e+00, -2.3547e+00,  1.4447e-01,  5.8983e-04,  2.6531e-02,\n","        -4.4454e-01,  4.5672e-01, -5.1684e-01, -1.8215e+00,  3.1645e-01,\n","         1.9048e-01, -2.3780e-01, -6.2056e-02, -8.1494e-01, -4.8422e-01,\n","         5.5276e-02, -3.7226e-02, -5.0315e-01,  1.1180e+00,  2.8110e-01,\n","        -1.7067e+00,  2.4674e-01,  6.7344e-01, -6.8665e-01, -1.1706e+00,\n","        -1.3717e-01, -1.3883e-01,  1.2846e+00, -4.0096e-01, -1.1797e+00,\n","        -8.4900e-01, -3.4966e-02,  6.2548e-01, -7.0700e-02, -6.8555e-01,\n","        -9.2926e-01, -1.9503e+00, -5.7533e-01, -4.6106e-01,  3.2578e-02,\n","        -5.6527e-01, -5.2932e-01, -5.4037e-01,  3.3623e-01,  7.0888e-01,\n","         6.2422e-01, -6.3159e-01, -1.5166e-01, -2.9575e-01,  2.4829e-01,\n","        -4.6621e-02, -9.4932e-01,  3.4737e-01, -4.0844e-01,  2.3977e-01,\n","         9.5977e-01,  5.1540e-01,  2.2341e+00, -1.2717e+00, -2.1301e-01,\n","        -2.3854e-01,  1.0480e+00,  1.1590e+00,  9.0410e-01,  1.6128e-02,\n","         5.4683e-01, -3.7037e-01,  3.2357e-01,  1.2470e+00, -1.5525e+00,\n","        -1.1780e+00,  7.2846e-01,  1.6992e-01, -4.1500e-01,  1.2743e+00,\n","        -9.8174e-01,  2.1908e-01, -1.1944e+00,  1.4378e+00,  2.3005e+00,\n","         1.9885e-01, -1.4651e+00, -3.9526e-01,  6.8653e-02, -5.3436e-01,\n","        -5.9269e-01,  1.0377e+00, -7.8868e-01,  9.3128e-01, -5.6315e-01,\n","         1.7324e-01, -8.0749e-01,  5.4697e-02, -8.1143e-01, -9.7398e-01,\n","        -1.3897e+00, -3.7109e-01, -1.4674e+00,  1.4702e-01,  1.0153e+00,\n","         1.0172e-01,  1.2336e+00, -1.3297e-01, -6.3273e-01,  1.0797e+00,\n","         9.8308e-01,  3.3598e-01, -1.4750e-01, -9.2109e-01,  6.5003e-01,\n","         2.3264e+00, -6.4646e-02,  2.8833e-01, -3.7344e-02, -9.0113e-01,\n","         2.1700e+00, -5.4343e-01,  1.1403e+00, -2.1411e+00, -5.8939e-01,\n","        -1.2706e+00,  8.0468e-01,  1.0619e+00,  3.2019e-01,  1.1073e+00,\n","        -7.6114e-01,  6.6611e-01,  1.1607e+00, -1.2611e+00, -1.9433e+00,\n","        -1.2345e+00, -9.8156e-02,  1.5877e-01,  5.2428e-02,  5.0668e-01,\n","         1.0370e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 위생\n","tensor([-0.4038,  2.3004, -0.1446, -0.8432, -0.6512, -0.0352, -1.1727, -0.8092,\n","        -1.4001,  0.1943,  0.6055,  1.2647, -0.9253, -1.8001,  1.3018, -0.6294,\n","        -0.0851,  0.6564, -0.1084, -1.1281,  0.3208, -1.0484, -2.0105,  1.4984,\n","        -0.9278, -0.7438,  0.4212,  0.6820,  0.5133, -1.0619, -0.8204, -1.4712,\n","        -0.1232,  0.7451,  2.0961,  1.7356,  0.8912,  0.7826,  0.5234,  0.7551,\n","        -0.4807,  1.0606, -0.3567,  2.2797,  0.5148, -0.7629, -1.6354, -0.2241,\n","        -0.6014,  0.8568, -1.1029,  1.3434, -2.1832,  1.2514, -0.2000,  0.4209,\n","         0.0537, -0.4710, -0.2094,  0.7886, -0.4787, -1.8473, -0.2000,  1.2255,\n","         0.9568,  0.5551,  0.4881,  0.9574,  0.4604, -0.1243, -0.7864, -1.0902,\n","        -0.4291, -0.1805, -0.7049,  0.5590, -1.8138,  0.7910, -0.1233,  1.0480,\n","         0.7289, -0.6010, -0.6022,  0.0835,  0.9230,  0.8720,  0.2144, -0.6312,\n","        -0.4701, -0.6308,  1.6491,  0.3942, -1.6407, -1.3485,  0.9865, -0.4517,\n","        -0.7320,  0.4384, -0.0842,  0.6774,  1.4578,  0.3402,  1.7673,  1.3797,\n","        -2.4383,  0.3574,  0.1281, -0.9781, -0.7323, -0.1014,  0.2230,  0.5224,\n","         0.4329, -0.2520,  0.0343,  0.2414,  0.8659,  0.0398,  0.9536,  0.8970,\n","         0.9736, -0.0356,  1.5214, -0.6767, -0.2319, -1.4648,  0.7261, -1.1683,\n","         0.6629,  0.5516,  1.0900,  0.9072, -0.1560, -1.1353,  0.7199,  0.2280,\n","        -1.0140,  0.8170,  2.1771, -2.5426,  0.7439, -0.6428, -1.1480,  1.8054,\n","         0.1256,  1.3230,  0.9919,  0.2684, -0.3150, -0.1691, -0.0705, -0.9512,\n","        -1.5292,  1.9258, -1.6147, -0.0943,  0.2530, -0.8469,  0.5733,  0.3915,\n","         0.1516, -0.4408, -1.5211,  1.0972, -0.5819,  0.8206, -1.4767, -0.3762,\n","        -0.7766,  0.3814,  1.8285, -0.4271,  1.0688, -0.9348, -0.1162,  0.7055,\n","         0.1759,  0.1308,  0.1370, -1.5698,  0.2806, -2.1742, -0.3788, -0.0516,\n","        -0.2133, -0.6841,  0.3003,  2.3164, -0.1699,  0.7916, -1.7551,  0.1745,\n","        -1.3585, -0.6296, -0.4618,  0.4783, -0.9357, -0.5948, -1.1357,  0.5221,\n","         0.8212,  0.4103,  0.2286, -0.0560,  0.3321,  1.3962,  0.2819,  0.2181,\n","         0.0754,  1.2696, -0.2580,  0.5327,  0.5417,  1.2160, -0.7616,  0.0868,\n","         2.5548,  0.8957,  1.5871,  0.0580, -0.3503,  0.4659, -0.8625, -0.1444,\n","         0.0433, -1.8400, -1.0900, -0.7677,  0.0722, -0.4497,  0.8684,  2.0906,\n","         1.3004, -1.7771, -1.0247,  0.5353, -0.3070, -2.7286,  0.1601,  1.2287,\n","         0.4770, -0.4557,  0.1926,  0.9066, -0.0378, -0.7977,  0.5888, -1.2839,\n","         0.3950, -2.3135, -0.8291, -0.6228,  0.2404,  1.5265,  1.3560,  2.4473],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 가격\n","tensor([ 1.2936, -2.5887, -1.5470, -1.2324, -1.0259,  0.5355,  0.1732, -0.5327,\n","         1.8585, -1.5875, -0.9185,  0.3439,  0.5266, -1.5882,  1.0610, -0.3456,\n","         0.0289, -0.9211,  0.4556, -0.1689, -1.2024,  0.5040, -0.0637,  0.7718,\n","        -0.4697, -0.4689,  0.0225,  0.8889,  1.5858, -0.7511,  0.4663, -0.6950,\n","        -0.4937,  0.6431, -1.0317,  0.0325,  0.9090,  1.6174, -1.0340, -0.7086,\n","        -0.1547,  1.3267, -0.2545, -0.5878,  2.1944, -1.0383, -1.2672,  0.7243,\n","        -0.7223, -1.5570,  0.3268,  0.1777, -1.6496, -0.3925,  0.7663,  0.4909,\n","        -1.1309,  0.2555,  1.6063,  0.5615,  1.6675, -0.8713, -1.7816,  0.2395,\n","        -2.3592,  0.4987,  0.3348,  0.4958, -0.3022, -0.2535, -1.3842, -0.0829,\n","         1.4958,  0.3893,  0.8898, -0.6826, -0.4488, -2.2238,  0.3184, -0.5470,\n","         0.1569,  0.2672,  1.0402,  0.0567, -2.0537, -0.8512,  1.7827,  1.4090,\n","         0.8703,  0.1390, -1.9428, -0.5245, -1.1125, -0.2748,  2.8205, -0.5616,\n","        -0.3349,  0.1739,  1.4712, -1.1406,  0.1541, -1.2247,  0.8770,  0.6595,\n","         0.4382,  1.8853,  1.0618,  0.2782,  0.3226, -0.1941,  0.7715, -1.5743,\n","        -0.2798,  0.4868, -0.8167,  1.8293, -0.8785,  0.4771,  0.4598,  0.1058,\n","        -0.5896,  1.0253,  0.2378, -0.0995, -1.7640,  1.2120,  0.1032, -1.7891,\n","        -0.6038,  0.8097, -0.3922, -1.6034, -0.4618, -0.1047, -0.4870,  0.2625,\n","         0.8756, -0.7098,  1.6001, -0.5770,  1.2643,  0.1449, -0.0331, -1.2586,\n","        -0.6999,  0.0200,  2.4035, -0.1856, -0.9267,  2.5655, -1.1960,  0.1655,\n","        -0.7192,  0.3689,  1.2671, -0.8461,  0.9551, -1.3345,  0.4297,  0.7758,\n","         0.3657, -0.6070, -0.4127, -0.6559,  1.1625,  0.5556, -1.1328,  1.0166,\n","         0.3585,  2.2144, -0.3333, -0.7134,  0.8468, -0.8501, -0.2773,  0.1649,\n","         0.7583, -0.7327,  1.2642, -0.9896, -0.5886,  0.1444, -1.7278, -0.6576,\n","        -0.2944,  0.2940,  1.7099, -0.2525,  0.1646, -0.5270, -0.6481, -0.8702,\n","        -0.2216,  0.3320, -0.0406,  0.7054, -0.5488, -0.6690,  0.3832,  2.0559,\n","         1.0620,  0.2524,  1.2538,  0.1902, -1.1345,  0.7505,  0.6509, -0.4065,\n","         0.4411,  1.7412, -0.6086,  2.2690,  0.2957, -1.6511,  0.0220,  0.7589,\n","        -0.0573,  1.0881, -0.6714, -1.0391,  0.4185, -0.4447, -0.5742, -1.2090,\n","        -0.3320,  0.1466, -0.4455, -0.1952, -1.7101, -0.3226, -0.4532, -1.1235,\n","        -0.8624,  1.0023,  1.0951, -0.1208, -0.7748, -1.0652, -0.4273,  0.1320,\n","         1.6249,  1.3172,  0.3827, -0.6192,  0.3279,  0.5533, -2.0913, -0.8934,\n","        -0.4596,  0.0568,  0.6380, -0.1335,  0.5631, -0.2539, -1.2442, -0.0469],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D9iCqiMIeCWv"},"source":[],"execution_count":null,"outputs":[]}]}